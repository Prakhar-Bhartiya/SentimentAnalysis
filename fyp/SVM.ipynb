{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/user1/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/home/user1/Downloads/archive/training.1600000.processed.noemoticon.csv', encoding = 'latin', header=None, nrows=18000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding header to data\n",
    "data = data.rename(columns={0: 'target', 1: 'id', 2: 'TimeStamp', 3: 'query', 4: 'username', 5: 'content'})\n",
    "#Dropping unncessary columns\n",
    "data.drop(['id','TimeStamp','query'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>username</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [target, username, content]\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[data['target'] == '2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop all the neutral rows\n",
    "data.drop(data[data['target']==2].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>username</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17995</th>\n",
       "      <td>0</td>\n",
       "      <td>ntmachai</td>\n",
       "      <td>wish my czats would know that on sunday people...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17996</th>\n",
       "      <td>0</td>\n",
       "      <td>sophster07</td>\n",
       "      <td>going to sleep without chris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17997</th>\n",
       "      <td>0</td>\n",
       "      <td>ernie_cordell</td>\n",
       "      <td>Looking for my last freaking update. I deactiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17998</th>\n",
       "      <td>0</td>\n",
       "      <td>deanna456</td>\n",
       "      <td>Headed out for the club - almost broke my ankl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17999</th>\n",
       "      <td>0</td>\n",
       "      <td>davidrules04</td>\n",
       "      <td>@liamyoung hehe yeah I failed tho wasn't prepa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       target         username  \\\n",
       "0           0  _TheSpecialOne_   \n",
       "1           0    scotthamilton   \n",
       "2           0         mattycus   \n",
       "3           0          ElleCTF   \n",
       "4           0           Karoli   \n",
       "...       ...              ...   \n",
       "17995       0         ntmachai   \n",
       "17996       0       sophster07   \n",
       "17997       0    ernie_cordell   \n",
       "17998       0        deanna456   \n",
       "17999       0     davidrules04   \n",
       "\n",
       "                                                 content  \n",
       "0      @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1      is upset that he can't update his Facebook by ...  \n",
       "2      @Kenichan I dived many times for the ball. Man...  \n",
       "3        my whole body feels itchy and like its on fire   \n",
       "4      @nationwideclass no, it's not behaving at all....  \n",
       "...                                                  ...  \n",
       "17995  wish my czats would know that on sunday people...  \n",
       "17996                      going to sleep without chris   \n",
       "17997  Looking for my last freaking update. I deactiv...  \n",
       "17998  Headed out for the club - almost broke my ankl...  \n",
       "17999  @liamyoung hehe yeah I failed tho wasn't prepa...  \n",
       "\n",
       "[18000 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA CLEANING -- \n",
      "\n",
      "Tweets have been cleaned.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"DATA CLEANING -- \\n\")\n",
    "\n",
    "# emojis defined\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "         u\"\\U00002702-\\U000027B0\"\n",
    "         u\"\\U000024C2-\\U0001F251\"\n",
    "         \"]+\", flags=re.UNICODE)\n",
    "\n",
    "\n",
    "#This function replaces happy unicode emojis with \"happy\" and sad unicode emojis with \"sad\".\n",
    "def replace_emojis(t):\n",
    "\n",
    "    emoji_happy = [\"\\U0001F600\", \"\\U0001F601\", \"\\U0001F602\",\"\\U0001F603\",\"\\U0001F604\",\"\\U0001F605\", \"\\U0001F606\", \"\\U0001F607\", \"\\U0001F609\", \n",
    "                \"\\U0001F60A\", \"\\U0001F642\",\"\\U0001F643\",\"\\U0001F923\",r\"\\U0001F970\",\"\\U0001F60D\", r\"\\U0001F929\",\"\\U0001F618\",\"\\U0001F617\",\n",
    "                r\"\\U000263A\", \"\\U0001F61A\", \"\\U0001F619\", r\"\\U0001F972\", \"\\U0001F60B\", \"\\U0001F61B\", \"\\U0001F61C\", r\"\\U0001F92A\",\n",
    "                \"\\U0001F61D\", \"\\U0001F911\", \"\\U0001F917\", r\"\\U0001F92D\", r\"\\U0001F92B\",\"\\U0001F914\",\"\\U0001F910\", r\"\\U0001F928\", \"\\U0001F610\", \"\\U0001F611\",\n",
    "                \"\\U0001F636\", \"\\U0001F60F\",\"\\U0001F612\", \"\\U0001F644\",\"\\U0001F62C\",\"\\U0001F925\",\"\\U0001F60C\",\"\\U0001F614\",\"\\U0001F62A\",\n",
    "                \"\\U0001F924\",\"\\U0001F634\", \"\\U0001F920\", r\"\\U0001F973\", r\"\\U0001F978\",\"\\U0001F60E\",\"\\U0001F913\", r\"\\U0001F9D0\"]\n",
    "\n",
    "    emoji_sad = [\"\\U0001F637\",\"\\U0001F912\",\"\\U0001F915\",\"\\U0001F922\", r\"\\U0001F92E\",\"\\U0001F927\", r\"\\U0001F975\", r\"\\U0001F976\", r\"\\U0001F974\",\n",
    "                       \"\\U0001F635\", r\"\\U0001F92F\", \"\\U0001F615\",\"\\U0001F61F\",\"\\U0001F641\", r\"\\U0002639\",\"\\U0001F62E\",\"\\U0001F62F\",\"\\U0001F632\",\n",
    "                       \"\\U0001F633\", r\"\\U0001F97A\",\"\\U0001F626\",\"\\U0001F627\",\"\\U0001F628\",\"\\U0001F630\",\"\\U0001F625\",\"\\U0001F622\",\"\\U0001F62D\",\n",
    "                       \"\\U0001F631\",\"\\U0001F616\",\"\\U0001F623\"\t,\"\\U0001F61E\",\"\\U0001F613\",\"\\U0001F629\",\"\\U0001F62B\", r\"\\U0001F971\",\n",
    "                       \"\\U0001F624\",\"\\U0001F621\",\"\\U0001F620\", r\"\\U0001F92C\",\"\\U0001F608\",\"\\U0001F47F\",\"\\U0001F480\", r\"\\U0002620\"]\n",
    "\n",
    "    words = t.split()\n",
    "    reformed = []\n",
    "    for w in words:\n",
    "        if w in emoji_happy:\n",
    "              reformed.append(\"happy\")\n",
    "        elif w in emoji_sad:\n",
    "              reformed.append(\"sad\") \n",
    "        else:\n",
    "              reformed.append(w)\n",
    "    t = \" \".join(reformed)\n",
    "    return t\n",
    "#This function replaces happy smileys with \"happy\" and sad smileys with \"sad.\n",
    "def replace_smileys(t):\n",
    "    \n",
    "    emoticons_happy = set([':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}', ':D',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)', '<3'])\n",
    "\n",
    "    emoticons_sad = set([':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('])  \n",
    "\n",
    "    words = t.split()\n",
    "    reformed = []\n",
    "    for w in words:\n",
    "        if w in emoticons_happy:\n",
    "              reformed.append(\"happy\")\n",
    "        elif w in emoticons_sad:\n",
    "              reformed.append(\"sad\") \n",
    "        else:\n",
    "              reformed.append(w)\n",
    "    t = \" \".join(reformed)\n",
    "    return t\n",
    "\n",
    "\n",
    "#This function replaces english lanuage contractions like \"shouldn't\" with \"should not\"\n",
    "def replace_contractions(t):\n",
    "\n",
    "    cont = {\"aren't\" : 'are not', \"can't\" : 'cannot', \"couln't\": 'could not', \"didn't\": 'did not', \"doesn't\" : 'does not',\n",
    "  \"hadn't\": 'had not', \"haven't\": 'have not', \"he's\" : 'he is', \"she's\" : 'she is', \"he'll\" : \"he will\", \n",
    "  \"she'll\" : 'she will',\"he'd\": \"he would\", \"she'd\":\"she would\", \"here's\" : \"here is\", \n",
    "   \"i'm\" : 'i am', \"i've\"\t: \"i have\", \"i'll\" : \"i will\", \"i'd\" : \"i would\", \"isn't\": \"is not\", \n",
    "   \"it's\" : \"it is\", \"it'll\": \"it will\", \"mustn't\" : \"must not\", \"shouldn't\" : \"should not\", \"that's\" : \"that is\", \n",
    "   \"there's\" : \"there is\", \"they're\" : \"they are\", \"they've\" : \"they have\", \"they'll\" : \"they will\",\n",
    "   \"they'd\" : \"they would\", \"wasn't\" : \"was not\", \"we're\": \"we are\", \"we've\":\"we have\", \"we'll\": \"we will\", \n",
    "   \"we'd\" : \"we would\", \"weren't\" : \"were not\", \"what's\" : \"what is\", \"where's\" : \"where is\", \"who's\": \"who is\",\n",
    "   \"who'll\" :\"who will\", \"won't\":\"will not\", \"wouldn't\" : \"would not\", \"you're\": \"you are\", \"you've\":\"you have\",\n",
    "   \"you'll\" : \"you will\", \"you'd\" : \"you would\", \"mayn't\" : \"may not\",\"u\":\"you\"}\n",
    "    words = t.split()\n",
    "    reformed = []\n",
    "    for w in words:\n",
    "        if w in cont:\n",
    "              reformed.append(cont[w])\n",
    "        else:\n",
    "              reformed.append(w)\n",
    "    t = \" \".join(reformed)\n",
    "    return t  \n",
    "\n",
    "def replace_slang(t):\n",
    "\n",
    "    slang = {\"afaik\" : \"as far as i know\",  \"$\" : \" dollar \",\"â‚¬\" : \" euro \", \"4ao\" : \"for adults only\",     \"a.m\" : \"before midday\",  \"aamof\" : \"as a matter of fact\",     \"acct\" : \"account\",     \"adih\" : \"another day in hell\",     \"afaic\" : \"as far as i am concerned\",     \"afaict\" : \"as far as i can tell\",     \"afaik\" : \"as far as i know\",     \"afair\" : \"as far as i remember\",     \"afk\" : \"away from keyboard\",     \"app\" : \"application\",     \"approx\" : \"approximately\",     \"apps\" : \"applications\",     \"asap\" : \"as soon as possible\",     \"asl\" : \"age, sex, location\",     \"atk\" : \"at the keyboard\",     \"ave.\" : \"avenue\",     \"aymm\" : \"are you my mother\",     \"ayor\" : \"at your own risk\",      \"b&b\" : \"bed and breakfast\",     \"b+b\" : \"bed and breakfast\",     \"b.c\" : \"before christ\",     \"b2b\" : \"business to business\",     \"b2c\" : \"business to customer\",     \"b4\" : \"before\",     \"b4n\" : \"bye for now\",     \"b@u\" : \"back at you\",     \"bae\" : \"before anyone else\",     \"bak\" : \"back at keyboard\",     \"bbbg\" : \"bye bye be good\",     \"bbc\" : \"british broadcasting corporation\",     \"bbias\" : \"be back in a second\",     \"bbl\" : \"be back later\",     \"bbs\" : \"be back soon\",     \"be4\" : \"before\",     \"bfn\" : \"bye for now\",     \"blvd\" : \"boulevard\",     \"bout\" : \"about\",     \"brb\" : \"be right back\",     \"bros\" : \"brothers\",     \"brt\" : \"be right there\",     \"bsaaw\" : \"big smile and a wink\",     \"btw\" : \"by the way\",     \"bwl\" : \"bursting with laughter\",     \"c/o\" : \"care of\",     \"cet\" : \"central european time\",     \"cf\" : \"compare\",     \"cia\" : \"central intelligence agency\",     \"csl\" : \"can not stop laughing\",     \"cu\" : \"see you\",     \"cul8r\" : \"see you later\",     \"cv\" : \"curriculum vitae\",     \"cwot\" : \"complete waste of time\",     \"cya\" : \"see you\",     \"cyt\" : \"see you tomorrow\",     \"dae\" : \"does anyone else\",     \"dbmib\" : \"do not bother me i am busy\",     \"diy\" : \"do it yourself\",     \"dm\" : \"direct message\",     \"dwh\" : \"during work hours\",     \"e123\" : \"easy as one two three\",     \"eet\" : \"eastern european time\",     \"eg\" : \"example\",     \"embm\" : \"early morning business meeting\",     \"encl\" : \"enclosed\",     \"encl.\" : \"enclosed\",     \"etc\" : \"and so on\",     \"faq\" : \"frequently asked questions\",     \"fawc\" : \"for anyone who cares\",     \"fb\" : \"facebook\",     \"fc\" : \"fingers crossed\",     \"fig\" : \"figure\",     \"fimh\" : \"forever in my heart\",      \"ft.\" : \"feet\",     \"ft\" : \"featuring\",     \"ftl\" : \"for the loss\",     \"ftw\" : \"for the win\",     \"fwiw\" : \"for what it is worth\",     \"fyi\" : \"for your information\",     \"g9\" : \"genius\",     \"gahoy\" : \"get a hold of yourself\",     \"gal\" : \"get a life\",     \"gcse\" : \"general certificate of secondary education\",     \"gfn\" : \"gone for now\",     \"gg\" : \"good game\",     \"gl\" : \"good luck\",     \"glhf\" : \"good luck have fun\",     \"gmt\" : \"greenwich mean time\",     \"gmta\" : \"great minds think alike\",     \"gn\" : \"good night\",     \"g.o.a.t\" : \"greatest of all time\",     \"goat\" : \"greatest of all time\",     \"goi\" : \"get over it\",     \"gps\" : \"global positioning system\",     \"gr8\" : \"great\",     \"gratz\" : \"congratulations\",     \"gyal\" : \"girl\",     \"h&c\" : \"hot and cold\",     \"hp\" : \"horsepower\",     \"hr\" : \"hour\",     \"hrh\" : \"his royal highness\",     \"ht\" : \"height\",     \"ibrb\" : \"i will be right back\",     \"icq\" : \"i seek you\",     \"icymi\" : \"in case you missed it\",     \"idc\" : \"i do not care\",     \"idgadf\" : \"i do not give a damn fuck\",     \"idgaf\" : \"i do not give a fuck\",     \"idk\" : \"i do not know\",     \"ie\" : \"that is\",     \"i.e\" : \"that is\",     \"ifyp\" : \"i feel your pain\",     \"IG\" : \"instagram\",     \"iirc\" : \"if i remember correctly\",  \"imho\" : \"in my humble opinion\",     \"imo\" : \"in my opinion\",     \"imu\" : \"i miss you\",     \"iow\" : \"in other words\",     \"irl\" : \"in real life\",     \"j4f\" : \"just for fun\",     \"jic\" : \"just in case\",     \"jk\" : \"just kidding\",     \"jsyk\" : \"just so you know\",     \"l8r\" : \"later\",     \"lb\" : \"pound\",     \"lbs\" : \"pounds\",     \"ldr\" : \"long distance relationship\",     \"lmao\" : \"laugh my ass off\",     \"lmfao\" : \"laugh my fucking ass off\",     \"lol\" : \"laughing out loud\",     \"ltd\" : \"limited\",     \"ltns\" : \"long time no see\",     \"m8\" : \"mate\",     \"mf\" : \"motherfucker\",     \"mfs\" : \"motherfuckers\",     \"mfw\" : \"my face when\",     \"mofo\" : \"motherfucker\",     \"mph\" : \"miles per hour\",     \"mr\" : \"mister\",     \"mrw\" : \"my reaction when\",     \"ms\" : \"miss\",     \"mte\" : \"my thoughts exactly\",     \"nagi\" : \"not a good idea\",     \"nbc\" : \"national broadcasting company\",     \"nbd\" : \"not big deal\",     \"nfs\" : \"not for sale\",     \"ngl\" : \"not going to lie\",     \"nhs\" : \"national health service\",     \"nrn\" : \"no reply necessary\",     \"nsfl\" : \"not safe for life\",     \"nsfw\" : \"not safe for work\",     \"nth\" : \"nice to have\",     \"nvr\" : \"never\",     \"nyc\" : \"new york city\",     \"oc\" : \"original content\",     \"og\" : \"original\",     \"ohp\" : \"overhead projector\",     \"oic\" : \"oh i see\",     \"omdb\" : \"over my dead body\",     \"omg\" : \"oh my god\",     \"omw\" : \"on my way\",     \"p.a\" : \"per annum\",     \"p.m\" : \"after midday\",     \"pm\" : \"prime minister\",     \"poc\" : \"people of color\",     \"pov\" : \"point of view\",     \"pp\" : \"pages\",     \"ppl\" : \"people\",     \"prw\" : \"parents are watching\",     \"ps\" : \"postscript\",     \"pt\" : \"point\",     \"ptb\" : \"please text back\",     \"pto\" : \"please turn over\",     \"qpsa\" : \"what happens\",      \"ratchet\" : \"rude\",     \"rbtl\" : \"read between the lines\",     \"rlrt\" : \"real life retweet\",      \"rofl\" : \"rolling on the floor laughing\",     \"roflol\" : \"rolling on the floor laughing out loud\",     \"rotflmao\" : \"rolling on the floor laughing my ass off\",  \"rt\" : \"retweet\",     \"ruok\" : \"are you ok\",     \"sfw\" : \"safe for work\", \"smh\" : \"shake my head\",     \"sq\" : \"square\",     \"srsly\" : \"seriously\",      \"ssdd\" : \"same stuff different day\",     \"tbh\" : \"to be honest\",     \"tbs\" : \"tablespooful\",     \"tbsp\" : \"tablespooful\",     \"tfw\" : \"that feeling when\",     \"thks\" : \"thank you\",     \"tho\" : \"though\",     \"thx\" : \"thank you\",     \"tia\" : \"thanks in advance\",     \"til\" : \"today i learned\",     \"tl;dr\" : \"too long i did not read\",     \"tldr\" : \"too long i did not read\",     \"tmb\" : \"tweet me back\",     \"tntl\" : \"trying not to laugh\",     \"ttyl\" : \"talk to you later\",     \"u\" : \"you\",     \"u2\" : \"you too\",     \"u4e\" : \"yours for ever\",     \"utc\" : \"coordinated universal time\",     \"w/\" : \"with\",     \"w/o\" : \"without\",     \"w8\" : \"wait\",     \"wassup\" : \"what is up\",     \"wb\" : \"welcome back\",     \"wtf\" : \"what the fuck\",     \"wtg\" : \"way to go\",     \"wtpa\" : \"where the party at\",     \"wuf\" : \"where are you from\",     \"wuzup\" : \"what is up\",     \"wywh\" : \"wish you were here\",     \"yd\" : \"yard\",     \"ygtr\" : \"you got that right\",     \"ynk\" : \"you never know\",     \"zzz\" : \"sleeping bored and tired\",  \"afk\" : \"away from keyboard\", \"asap\" : \"as soon as possible\", \"atk\" : \"at the keyboard\", \"atm\" : \"at the moment\", \"a3\" : \"anytime, anywhere, anyplace\", \"bak\" : \"back at keyboard\", \"bbl\" : \"be back later\", \"bbs\" : \"be back soon\", \"bfn\" : \"bye for now\", \"brb\" : \"be right back\", \"brt\" : \"be right there\", \"btw\" : \"by the way\", \"b4\" : \"before\", \"b4n\" : \"bye for now\", \"cu\" : \"see you\", \"cul8r\" : \"see you later\", \"cya\" : \"see you\", \"faq\" : \"frequently asked questions\", \"fc\" : \"fingers crossed\", \"fwiw\" : \"for what it's worth\", \"fyi\" : \"for your information\", \"gal\" : \"get a life\", \"gg\" : \"good game\", \"gn\" : \"good night\", \"gmta\" : \"great minds think alike\", \"gr8\" : \"great!\", \"g9\" : \"genius\", \"ic\" : \"i see\", \"icq\" : \"i seek you (also a chat program)\",\"ilu\": \"i love you\", \"imho\" : \"in my honest/humble opinion\", \"imo\" : \"in my opinion\", \"iow\" : \"in other words\", \"irl\" : \"in real life\", \"kiss\" : \"keep it simple, stupid\", \"ldr\" : \"long distance relationship\", \"lmao\" : \"laugh my a.. off\", \"lol\" : \"laughing out loud\", \"ltns\" : \"long time no see\", \"luv\" : \"love\", \"l8r\" : \"later\", \"mte\" : \"my thoughts exactly\", \"m8\" : \"mate\", \"nrn\" : \"no reply necessary\", \"oic\" : \"oh i see\", \"pita\" : \"pain in the a..\", \"prt\" : \"party\", \"prw\" : \"parents are watching\",\"rofl\" : \"rolling on the floor laughing\", \"roflol\" : \"rolling on the floor laughing out loud\", \"rotflmao\" : \"rolling on the floor laughing my a.. off\", \"sk8\" : \"skate\", \"stats\" : \"your sex and age\", \"asl\" : \"age, sex, location\", \"thx\" : \"thank you\", \"ttfn\" : \"ta-ta for now!\", \"ttyl\" : \"talk to you later\", \"u\" : \"you\", \"u2\" : \"you too\", \"u4e\" : \"yours for ever\", \"wb\" : \"welcome back\", \"wtf\" : \"what the f...\", \"wtg\" : \"way to go!\", \"wuf\" : \"where are you from?\", \"w8\" : \"wait\", \"rn\":\"right now\",\"ya\":\"yes\",\"yeah\":\"yes\",\"ur\":\"you are\",\"ok\":\"okay\"}\n",
    "                \n",
    "    words = t.split()\n",
    "    reformed = []\n",
    "    for w in words:\n",
    "        if w in slang:\n",
    "              reformed.append(slang[w])\n",
    "        else:\n",
    "              reformed.append(w)\n",
    "    t = \" \".join(reformed)\n",
    "    return t  \n",
    "\n",
    "\n",
    "#This function removes words that are single characters\n",
    "def remove_single_letter_words(t):\n",
    "    words = t.split()\n",
    "    reformed = []\n",
    "    for w in words:\n",
    "        if len(w) > 1:\n",
    "            reformed.append(w)\n",
    "    t = \" \".join(reformed)\n",
    "    return t  \n",
    "\n",
    "def dataclean(t):\n",
    "\n",
    "    t = replace_smileys(t) # replace handwritten emojis with their feeling associated\n",
    "    t = t.lower() # convert to lowercase\n",
    "    t = replace_contractions(t) # replace short forms used in english  with their actual words\n",
    "    t = replace_slang(t)\n",
    "    t = replace_emojis(t) # replace unicode emojis with their feeling associated\n",
    "    t = emoji_pattern.sub(r'', t) # remove emojis other than smiley emojis\n",
    "    t = re.sub('\\\\\\\\u[0-9A-Fa-f]{4}','', t) # remove NON- ASCII characters\n",
    "    t = re.sub(\"[0-9]\", \"\", t) # remove numbers # re.sub(\"\\d+\", \"\", t)\n",
    "    t = re.sub('#', '', t) # remove '#'\n",
    "    t = re.sub('@[A-Za-z0â€“9]+', '', t) # remove '@'\n",
    "    t = re.sub('@[^\\s]+', '', t) # remove usernames\n",
    "    t = re.sub('RT[\\s]+', '', t) # remove retweet 'RT'\n",
    "    t = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', '', t) # remove links (URLs/ links)\n",
    "    t = re.sub('[!\"$%&\\'()*+,-./:@;<=>?[\\\\]^_`{|}~]', '', t) # remove punctuations\n",
    "    t = t.replace('\\\\\\\\', '')\n",
    "    t = t.replace('\\\\', '')\n",
    "    t = remove_single_letter_words(t) # removes single letter words\n",
    "  \n",
    "    return t\n",
    "\n",
    "data['content'] = data['content'].apply(dataclean)\n",
    "print(\"Tweets have been cleaned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLTK\n",
    "english_stopwords = stopwords.words('english')\n",
    "#base of english stopwords\n",
    "stemmer = SnowballStemmer('english')\n",
    "#stemming algorithm\n",
    "regex = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "#regex for mentions and links in tweets\n",
    "\n",
    "def preprocess(content, stem=False):\n",
    "    content = re.sub(regex, ' ', str(content).lower()).strip()\n",
    "    tokens = []\n",
    "    for token in content.split():\n",
    "        if token not in english_stopwords:\n",
    "              tokens.append(stemmer.stem(token))\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "data.content = data.content.apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>username</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>awww bummer shoulda got david carr third day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>upset cannot updat facebook text might cri res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>dive mani time ball manag save rest go bound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>whole bodi feel itchi like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>behav mad cannot see</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         username                                            content\n",
       "0       0  _TheSpecialOne_       awww bummer shoulda got david carr third day\n",
       "1       0    scotthamilton  upset cannot updat facebook text might cri res...\n",
       "2       0         mattycus       dive mani time ball manag save rest go bound\n",
       "3       0          ElleCTF                    whole bodi feel itchi like fire\n",
       "4       0           Karoli                               behav mad cannot see"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>18000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        target\n",
       "count  18000.0\n",
       "mean       0.0\n",
       "std        0.0\n",
       "min        0.0\n",
       "25%        0.0\n",
       "50%        0.0\n",
       "75%        0.0\n",
       "max        0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#import matplotlib.pyplot as plt\n",
    "import re\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import nltk\n",
    "\n",
    "#import preprocessor as p\n",
    "\n",
    "#from gensim.models import KeyedVectors\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from sklearn.cluster import KMeans\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "               ])\n",
    "sgd.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(tdidf_tensor, df['label'].values, test_size=0.3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
