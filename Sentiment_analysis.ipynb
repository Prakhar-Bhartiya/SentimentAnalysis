{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f75add9",
   "metadata": {},
   "source": [
    "\n",
    "# DATA DESCRIPTION\n",
    "\n",
    "**Sentiment140 dataset. It contains 1,600,000 tweets extracted using the twitter api.**\n",
    "\n",
    "It contains the following 6 fields:\n",
    "\n",
    "target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "ids: The id of the tweet ( 2087)\n",
    "date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "flag: The query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "user: the user that tweeted (robotickilldozr)\n",
    "text: the text of the tweet (Lyx is cool)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50069d84",
   "metadata": {},
   "source": [
    "**Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1010d433",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/prakharbhartiya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "#Nltk\n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3ed0acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/training.1600000.processed.noemoticon.csv', encoding = 'latin', header=None, nrows=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd5d9186",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding header to data\n",
    "data = data.rename(columns={0: 'target', 1: 'id', 2: 'TimeStamp', 3: 'query', 4: 'username', 5: 'content'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b245f6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping unncessary columns\n",
    "data.drop(['id','TimeStamp','query'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19132c0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>username</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         username                                            content\n",
       "0       0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1       0    scotthamilton  is upset that he can't update his Facebook by ...\n",
       "2       0         mattycus  @Kenichan I dived many times for the ball. Man...\n",
       "3       0          ElleCTF    my whole body feels itchy and like its on fire \n",
       "4       0           Karoli  @nationwideclass no, it's not behaving at all...."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a476d959",
   "metadata": {},
   "source": [
    "**Cleaning Data**\n",
    "Stemming - [Running, Runned, Runner] all can reduce to the stem Run. Below we have used the base of english stopwords and stemming algorithm from nltk library. \n",
    "\n",
    "Converting Unicode Emojis into Happy or Sad\n",
    "\n",
    "![Emojis code](img/emoji_data_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e0252f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA CLEANING -- \n",
      "\n",
      "Cleaning the tweets from the data.\n",
      "\n",
      "Replacing handwritten emojis with their feeling associated.\n",
      "Convert to lowercase.\n",
      "Replace contractions.\n",
      "Replace unicode emojis with their feeling associated.\n",
      "Remove all other unicoded emojis.\n",
      "Remove NON- ASCII characters.\n",
      "Remove numbers.\n",
      "Remove \"#\". \n",
      "Remove \"@\". \n",
      "Remove usernames.\n",
      "Remove 'RT'. \n",
      "Replace all URLs and Links with word 'URL'.\n",
      "Remove all punctuations.\n",
      "Removes single letter words.\n",
      "\n",
      "Tweets have been cleaned.\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "print(\"DATA CLEANING -- \\n\")\n",
    "\n",
    "# emojis defined\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "         u\"\\U00002702-\\U000027B0\"\n",
    "         u\"\\U000024C2-\\U0001F251\"\n",
    "         \"]+\", flags=re.UNICODE)\n",
    "\n",
    "\n",
    "#This function replaces happy unicode emojis with \"happy\" and sad unicode emojis with \"sad\".\n",
    "def replace_emojis(t):\n",
    "\n",
    "    emoji_happy = [\"\\U0001F600\", \"\\U0001F601\", \"\\U0001F602\",\"\\U0001F603\",\"\\U0001F604\",\"\\U0001F605\", \"\\U0001F606\", \"\\U0001F607\", \"\\U0001F609\", \n",
    "                \"\\U0001F60A\", \"\\U0001F642\",\"\\U0001F643\",\"\\U0001F923\",r\"\\U0001F970\",\"\\U0001F60D\", r\"\\U0001F929\",\"\\U0001F618\",\"\\U0001F617\",\n",
    "                r\"\\U000263A\", \"\\U0001F61A\", \"\\U0001F619\", r\"\\U0001F972\", \"\\U0001F60B\", \"\\U0001F61B\", \"\\U0001F61C\", r\"\\U0001F92A\",\n",
    "                \"\\U0001F61D\", \"\\U0001F911\", \"\\U0001F917\", r\"\\U0001F92D\", r\"\\U0001F92B\",\"\\U0001F914\",\"\\U0001F910\", r\"\\U0001F928\", \"\\U0001F610\", \"\\U0001F611\",\n",
    "                \"\\U0001F636\", \"\\U0001F60F\",\"\\U0001F612\", \"\\U0001F644\",\"\\U0001F62C\",\"\\U0001F925\",\"\\U0001F60C\",\"\\U0001F614\",\"\\U0001F62A\",\n",
    "                \"\\U0001F924\",\"\\U0001F634\", \"\\U0001F920\", r\"\\U0001F973\", r\"\\U0001F978\",\"\\U0001F60E\",\"\\U0001F913\", r\"\\U0001F9D0\"]\n",
    "\n",
    "    emoji_sad = [\"\\U0001F637\",\"\\U0001F912\",\"\\U0001F915\",\"\\U0001F922\", r\"\\U0001F92E\",\"\\U0001F927\", r\"\\U0001F975\", r\"\\U0001F976\", r\"\\U0001F974\",\n",
    "                       \"\\U0001F635\", r\"\\U0001F92F\", \"\\U0001F615\",\"\\U0001F61F\",\"\\U0001F641\", r\"\\U0002639\",\"\\U0001F62E\",\"\\U0001F62F\",\"\\U0001F632\",\n",
    "                       \"\\U0001F633\", r\"\\U0001F97A\",\"\\U0001F626\",\"\\U0001F627\",\"\\U0001F628\",\"\\U0001F630\",\"\\U0001F625\",\"\\U0001F622\",\"\\U0001F62D\",\n",
    "                       \"\\U0001F631\",\"\\U0001F616\",\"\\U0001F623\"\t,\"\\U0001F61E\",\"\\U0001F613\",\"\\U0001F629\",\"\\U0001F62B\", r\"\\U0001F971\",\n",
    "                       \"\\U0001F624\",\"\\U0001F621\",\"\\U0001F620\", r\"\\U0001F92C\",\"\\U0001F608\",\"\\U0001F47F\",\"\\U0001F480\", r\"\\U0002620\"]\n",
    "\n",
    "    words = t.split()\n",
    "    reformed = []\n",
    "    for w in words:\n",
    "        if w in emoji_happy:\n",
    "              reformed.append(\"happy\")\n",
    "        elif w in emoji_sad:\n",
    "              reformed.append(\"sad\") \n",
    "        else:\n",
    "              reformed.append(w)\n",
    "    t = \" \".join(reformed)\n",
    "    return t\n",
    "\n",
    "\n",
    "\n",
    "#This function replaces happy smileys with \"happy\" and sad smileys with \"sad.\n",
    "def replace_smileys(t):\n",
    "    \n",
    "    emoticons_happy = set([':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}', ':D',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)', '<3'])\n",
    "\n",
    "    emoticons_sad = set([':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('])  \n",
    "\n",
    "    words = t.split()\n",
    "    reformed = []\n",
    "    for w in words:\n",
    "        if w in emoticons_happy:\n",
    "              reformed.append(\"happy\")\n",
    "        elif w in emoticons_sad:\n",
    "              reformed.append(\"sad\") \n",
    "        else:\n",
    "              reformed.append(w)\n",
    "    t = \" \".join(reformed)\n",
    "    return t\n",
    "\n",
    "\n",
    "#This function replaces english lanuage contractions like \"shouldn't\" with \"should not\"\n",
    "def replace_contractions(t):\n",
    "\n",
    "    cont = {\"aren't\" : 'are not', \"can't\" : 'cannot', \"couln't\": 'could not', \"didn't\": 'did not', \"doesn't\" : 'does not',\n",
    "  \"hadn't\": 'had not', \"haven't\": 'have not', \"he's\" : 'he is', \"she's\" : 'she is', \"he'll\" : \"he will\", \n",
    "  \"she'll\" : 'she will',\"he'd\": \"he would\", \"she'd\":\"she would\", \"here's\" : \"here is\", \n",
    "   \"i'm\" : 'i am', \"i've\"\t: \"i have\", \"i'll\" : \"i will\", \"i'd\" : \"i would\", \"isn't\": \"is not\", \n",
    "   \"it's\" : \"it is\", \"it'll\": \"it will\", \"mustn't\" : \"must not\", \"shouldn't\" : \"should not\", \"that's\" : \"that is\", \n",
    "   \"there's\" : \"there is\", \"they're\" : \"they are\", \"they've\" : \"they have\", \"they'll\" : \"they will\",\n",
    "   \"they'd\" : \"they would\", \"wasn't\" : \"was not\", \"we're\": \"we are\", \"we've\":\"we have\", \"we'll\": \"we will\", \n",
    "   \"we'd\" : \"we would\", \"weren't\" : \"were not\", \"what's\" : \"what is\", \"where's\" : \"where is\", \"who's\": \"who is\",\n",
    "   \"who'll\" :\"who will\", \"won't\":\"will not\", \"wouldn't\" : \"would not\", \"you're\": \"you are\", \"you've\":\"you have\",\n",
    "   \"you'll\" : \"you will\", \"you'd\" : \"you would\", \"mayn't\" : \"may not\"}\n",
    "    words = t.split()\n",
    "    reformed = []\n",
    "    for w in words:\n",
    "        if w in cont:\n",
    "              reformed.append(cont[w])\n",
    "        else:\n",
    "              reformed.append(w)\n",
    "    t = \" \".join(reformed)\n",
    "    return t  \n",
    "\n",
    "\n",
    "#This function removes words that are single characters\n",
    "def remove_single_letter_words(t):\n",
    "    words = t.split()\n",
    "    reformed = []\n",
    "    for w in words:\n",
    "        if len(w) > 1:\n",
    "            reformed.append(w)\n",
    "    t = \" \".join(reformed)\n",
    "    return t  \n",
    "\n",
    "print(\"Cleaning the tweets from the data.\\n\")\n",
    "print(\"Replacing handwritten emojis with their feeling associated.\")\n",
    "print(\"Convert to lowercase.\")\n",
    "print(\"Replace contractions.\")\n",
    "print(\"Replace unicode emojis with their feeling associated.\")\n",
    "print(\"Remove all other unicoded emojis.\")\n",
    "print(\"Remove NON- ASCII characters.\")\n",
    "print(\"Remove numbers.\")\n",
    "print(\"Remove \\\"#\\\". \")\n",
    "print(\"Remove \\\"@\\\". \")\n",
    "print(\"Remove usernames.\")\n",
    "print(\"Remove \\'RT\\'. \")\n",
    "print(\"Replace all URLs and Links with word \\'URL\\'.\")\n",
    "print(\"Remove all punctuations.\")\n",
    "print(\"Removes single letter words.\\n\")\n",
    "\n",
    "#This function cleans the tweets. (Main Function)\n",
    "def dataclean(t):\n",
    "\n",
    "    t = replace_smileys(t) # replace handwritten emojis with their feeling associated\n",
    "    t = t.lower() # convert to lowercase\n",
    "    t = replace_contractions(t) # replace short forms used in english  with their actual words\n",
    "    t = replace_emojis(t) # replace unicode emojis with their feeling associated\n",
    "    t = emoji_pattern.sub(r'', t) # remove emojis other than smiley emojis\n",
    "    t = re.sub('\\\\\\\\u[0-9A-Fa-f]{4}','', t) # remove NON- ASCII characters\n",
    "    t = re.sub(\"[0-9]\", \"\", t) # remove numbers # re.sub(\"\\d+\", \"\", t)\n",
    "    t = re.sub('#', '', t) # remove '#'\n",
    "    t = re.sub('@[A-Za-z0–9]+', '', t) # remove '@'\n",
    "    t = re.sub('@[^\\s]+', '', t) # remove usernames\n",
    "    t = re.sub('RT[\\s]+', '', t) # remove retweet 'RT'\n",
    "    t = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', '', t) # remove links (URLs/ links)\n",
    "    t = re.sub('[!\"$%&\\'()*+,-./:@;<=>?[\\\\]^_`{|}~]', '', t) # remove punctuations\n",
    "    t = t.replace('\\\\\\\\', '')\n",
    "    t = t.replace('\\\\', '')\n",
    "    t = remove_single_letter_words(t) # removes single letter words\n",
    "  \n",
    "    return t\n",
    "\n",
    "data['content'] = data['content'].apply(dataclean)\n",
    "print(\"Tweets have been cleaned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f130b7f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>username</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>awww that is bummer you shoulda got david carr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he cannot update his facebook by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>dived many times for the ball managed to save ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>no it is not behaving at all am mad why am her...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         username                                            content\n",
       "0       0  _TheSpecialOne_  awww that is bummer you shoulda got david carr...\n",
       "1       0    scotthamilton  is upset that he cannot update his facebook by...\n",
       "2       0         mattycus  dived many times for the ball managed to save ...\n",
       "3       0          ElleCTF     my whole body feels itchy and like its on fire\n",
       "4       0           Karoli  no it is not behaving at all am mad why am her..."
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e0c9bdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLTK\n",
    "english_stopwords = stopwords.words('english')\n",
    "#base of english stopwords\n",
    "stemmer = SnowballStemmer('english')\n",
    "#stemming algorithm\n",
    "regex = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "#regex for mentions and links in tweets\n",
    "\n",
    "def preprocess(content, stem=False):\n",
    "    content = re.sub(regex, ' ', str(content).lower()).strip()\n",
    "    tokens = []\n",
    "    for token in content.split():\n",
    "        if token not in english_stopwords:\n",
    "              tokens.append(stemmer.stem(token))\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "data.content = data.content.apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f14bdb87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of     target         username                                            content\n",
       "0        0  _TheSpecialOne_       awww bummer shoulda got david carr third day\n",
       "1        0    scotthamilton  upset cannot updat facebook text might cri res...\n",
       "2        0         mattycus       dive mani time ball manag save rest go bound\n",
       "3        0          ElleCTF                    whole bodi feel itchi like fire\n",
       "4        0           Karoli                               behav mad cannot see\n",
       "5        0         joy_wolf                                         whole crew\n",
       "6        0          mybirch                                           need hug\n",
       "7        0             coZZ  hey long time see yes rain bit bit lol fine th...\n",
       "8        0  2Hood4Hollywood                                               nope\n",
       "9        0          mimismo                                          que muera\n",
       "10       0   erinx3leannexo                       spring break plain citi snow\n",
       "11       0     pardonlauren                                        repierc ear\n",
       "12       0             TLeC       couldnt bear watch thought ua loss embarrass\n",
       "13       0  robrobbierobert                 count idk either never talk anymor\n",
       "14       0      bayofwolves  wouldv first gun realli though zac snyder douc...\n",
       "15       0       HairByJess                        wish got watch miss premier\n",
       "16       0   lovesongwriter  holli death scene hurt sever watch film wri di...\n",
       "17       0         armotley                                           file tax\n",
       "18       0       starkissed        ahh ive alway want see rent love soundtrack\n",
       "19       0        gi_gi_bee                 oh dear drink forgotten tabl drink\n",
       "20       0           quanvu                                  day get much done\n",
       "21       0       swinspeedx  one friend call ask meet mid valley todaybut t...\n",
       "22       0        cooliodoc                              barista bake cake ate\n",
       "23       0       viJILLante                                       week go hope\n",
       "24       0       Ljelli3166                               blagh class tomorrow>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53016cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
